{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6c050e-5678-4450-b8fc-98d887f215cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote summaries_russian/recognized_small_20m20s.txt.mbart.summary.txt, time: 105.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Wrote summaries_russian/recognized_small_20m20s.txt.rut5.summary.txt, time: 33.6s\n",
      "summa not installed, skipping textrank.\n",
      "  Wrote summaries_russian/recognized_medium_24m15s.txt.mbart.summary.txt, time: 77.4s\n",
      "  Wrote summaries_russian/recognized_medium_24m15s.txt.rut5.summary.txt, time: 22.0s\n",
      "summa not installed, skipping textrank.\n",
      "  Wrote summaries_russian/recognized_large_33m5s.txt.mbart.summary.txt, time: 81.9s\n",
      "  Wrote summaries_russian/recognized_large_33m5s.txt.rut5.summary.txt, time: 31.8s\n",
      "summa not installed, skipping textrank.\n",
      "  Wrote summaries_russian/recognized_tiny_14m19s.txt.mbart.summary.txt, time: 174.1s\n",
      "  Wrote summaries_russian/recognized_tiny_14m19s.txt.rut5.summary.txt, time: 74.8s\n",
      "summa not installed, skipping textrank.\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "TXTS_PATH = \"transcripts\"\n",
    "OUT_PATH = \"summaries_russian\"\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "\n",
    "CHUNK_TOKENS = 1024\n",
    "SUMMARY_TOKENS = 150\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "LANG_CODE = \"ru_RU\"\n",
    "\n",
    "def chunk_text(text, tokenizer, max_tokens):\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences = sent_tokenize(text, language='russian')\n",
    "    chunks, current = [], \"\"\n",
    "    for sent in sentences:\n",
    "        if len(tokenizer.encode(current + sent)) < max_tokens:\n",
    "            current += \" \" + sent\n",
    "        else:\n",
    "            if current: chunks.append(current)\n",
    "            current = sent\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    return chunks\n",
    "\n",
    "def summarize_with_mbart50(text, model, tokenizer):\n",
    "    chunks = chunk_text(text, tokenizer, CHUNK_TOKENS)\n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\n",
    "            chunk, return_tensors=\"pt\", max_length=CHUNK_TOKENS, truncation=True\n",
    "        ).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=SUMMARY_TOKENS, \n",
    "                num_beams=4, \n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[LANG_CODE]\n",
    "            )\n",
    "        out = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(out)\n",
    "    merged_summary = \" \".join(summaries)\n",
    "    return merged_summary\n",
    "\n",
    "def summarize_with_rut5(text, pipe):\n",
    "    chunks = chunk_text(text, pipe.tokenizer, CHUNK_TOKENS)\n",
    "    # ruT5 is T5, so use native summarization pipeline\n",
    "    summaries = pipe(chunks, max_length=SUMMARY_TOKENS, min_length=20, truncation=True, batch_size=4)\n",
    "    merged = \" \".join([out['summary_text'] for out in summaries])\n",
    "    return merged\n",
    "\n",
    "# LOAD MODELS ONCE\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\n",
    "mbart_tokenizer.src_lang = LANG_CODE\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt').to(DEVICE)\n",
    "\n",
    "ruT5_model_name = \"ai-forever/ruT5-base\"\n",
    "ruT5_pipe = pipeline(\"summarization\", model=ruT5_model_name, tokenizer=ruT5_model_name, device=DEVICE)\n",
    "\n",
    "timing_report = []  # List of dicts: one row per run\n",
    "\n",
    "for fname in os.listdir(TXTS_PATH):\n",
    "    if not fname.endswith(\".txt\"): continue\n",
    "    fpath = os.path.join(TXTS_PATH, fname)\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # MBART-50\n",
    "    outname_mbart = os.path.join(OUT_PATH, f\"{fname}.mbart.summary.txt\")\n",
    "    if not os.path.exists(outname_mbart):\n",
    "        t0 = time.time()\n",
    "        summary = summarize_with_mbart50(text, mbart_model, mbart_tokenizer)\n",
    "        elapsed = time.time() - t0\n",
    "        with open(outname_mbart, \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(summary)\n",
    "        print(f\"  Wrote {outname_mbart}, time: {elapsed:.1f}s\")\n",
    "        timing_report.append({'filename': fname, 'model': 'mbart50', 'seconds': elapsed})\n",
    "        \n",
    "    # ruT5\n",
    "    outname_rut5 = os.path.join(OUT_PATH, f\"{fname}.rut5.summary.txt\")\n",
    "    if not os.path.exists(outname_rut5):\n",
    "        t0 = time.time()\n",
    "        summary = summarize_with_rut5(text, ruT5_pipe)\n",
    "        elapsed = time.time() - t0\n",
    "        with open(outname_rut5, \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(summary)\n",
    "        print(f\"  Wrote {outname_rut5}, time: {elapsed:.1f}s\")\n",
    "        timing_report.append({'filename': fname, 'model': 'ruT5', 'seconds': elapsed})\n",
    "\n",
    "    # TextRank\n",
    "    try:\n",
    "        from summa.summarizer import summarize as textrank_summarize\n",
    "        outname_textrank = os.path.join(OUT_PATH, f\"{fname}.textrank.summary.txt\")\n",
    "        if not os.path.exists(outname_textrank):\n",
    "            t0 = time.time()\n",
    "            summary = textrank_summarize(text, language=\"russian\", ratio=0.07)\n",
    "            elapsed = time.time() - t0\n",
    "            with open(outname_textrank, \"w\", encoding=\"utf-8\") as fout:\n",
    "                fout.write(summary)\n",
    "            print(f\"  Wrote {outname_textrank}, time: {elapsed:.1f}s\")\n",
    "            timing_report.append({'filename': fname, 'model': 'textrank', 'seconds': elapsed})\n",
    "    except ImportError:\n",
    "        print(\"summa not installed, skipping textrank.\")\n",
    "\n",
    "\n",
    "print(\"DONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23add44-0188-44dc-bf5a-83c0e1a8e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing CSV report saved: summaries_russian/timing_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Save timing results for analysis\n",
    "with open(os.path.join(OUT_PATH, 'timing_report.csv'), \"w\", encoding=\"utf-8\", newline='') as f_csv:\n",
    "    writer = csv.DictWriter(f_csv, fieldnames=['filename', 'model', 'seconds'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(timing_report)\n",
    "print(\"Timing CSV report saved:\", os.path.join(OUT_PATH, 'timing_report.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dee90-864b-4d70-8610-33abc8b2bac8",
   "metadata": {},
   "source": [
    "# LLM chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f813141e-7b47-40a7-aebb-543c1b83c798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.yandex-team.ru/simple/, https://pypi.org/simple/\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.46.2)\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venv/lib/python3.10/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venv/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./venv/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate torch sentencepiece tqdm --extra-index-url https://pypi.org/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23911ae7-20bd-45a2-bc21-f42e0ec9c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading mistral (mistralai/Mistral-7B-Instruct-v0.2) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489a6ff41e6644e78d97f6a58e7dbb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):   0%|                                                                                    | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):   4%|██▊                                                                         | 1/27 [00:35<15:21, 35.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):   7%|█████▋                                                                      | 2/27 [01:22<17:41, 42.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  11%|████████▍                                                                   | 3/27 [01:58<15:47, 39.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  15%|███████████▎                                                                | 4/27 [02:37<15:03, 39.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  19%|██████████████                                                              | 5/27 [03:18<14:34, 39.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  22%|████████████████▉                                                           | 6/27 [03:58<13:56, 39.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  26%|███████████████████▋                                                        | 7/27 [04:46<14:09, 42.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  30%|██████████████████████▌                                                     | 8/27 [05:32<13:47, 43.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  33%|█████████████████████████▎                                                  | 9/27 [06:14<12:56, 43.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  37%|███████████████████████████▊                                               | 10/27 [06:50<11:37, 41.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  41%|██████████████████████████████▌                                            | 11/27 [07:30<10:48, 40.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  44%|█████████████████████████████████▎                                         | 12/27 [08:09<10:03, 40.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  48%|████████████████████████████████████                                       | 13/27 [08:30<08:00, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  52%|██████████████████████████████████████▉                                    | 14/27 [09:14<08:04, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  56%|█████████████████████████████████████████▋                                 | 15/27 [10:01<08:01, 40.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  59%|████████████████████████████████████████████▍                              | 16/27 [10:40<07:19, 40.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  63%|███████████████████████████████████████████████▏                           | 17/27 [11:19<06:34, 39.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  67%|██████████████████████████████████████████████████                         | 18/27 [11:57<05:51, 39.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  70%|████████████████████████████████████████████████████▊                      | 19/27 [12:35<05:10, 38.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  74%|███████████████████████████████████████████████████████▌                   | 20/27 [13:15<04:34, 39.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  78%|██████████████████████████████████████████████████████████▎                | 21/27 [13:54<03:53, 38.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  81%|█████████████████████████████████████████████████████████████              | 22/27 [14:33<03:15, 39.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  85%|███████████████████████████████████████████████████████████████▉           | 23/27 [15:10<02:33, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  89%|██████████████████████████████████████████████████████████████████▋        | 24/27 [15:46<01:53, 37.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  93%|█████████████████████████████████████████████████████████████████████▍     | 25/27 [16:23<01:15, 37.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral):  96%|████████████████████████████████████████████████████████████████████████▏  | 26/27 [17:02<00:37, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (mistral): 100%|███████████████████████████████████████████████████████████████████████████| 27/27 [17:26<00:00, 38.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_small_20m20s.txt (mistral) done in 1046.3 sec (27 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):   0%|                                                                                   | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):   4%|███▎                                                                       | 1/23 [00:13<04:58, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):   9%|██████▌                                                                    | 2/23 [00:54<10:21, 29.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  13%|█████████▊                                                                 | 3/23 [01:34<11:24, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  17%|█████████████                                                              | 4/23 [02:10<11:09, 35.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  22%|████████████████▎                                                          | 5/23 [03:00<12:05, 40.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  26%|███████████████████▌                                                       | 6/23 [03:49<12:15, 43.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  30%|██████████████████████▊                                                    | 7/23 [04:33<11:39, 43.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  35%|██████████████████████████                                                 | 8/23 [05:16<10:48, 43.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  39%|█████████████████████████████▎                                             | 9/23 [05:53<09:40, 41.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  43%|████████████████████████████████▏                                         | 10/23 [06:30<08:42, 40.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  48%|███████████████████████████████████▍                                      | 11/23 [07:08<07:51, 39.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  52%|██████████████████████████████████████▌                                   | 12/23 [07:36<06:36, 36.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  57%|█████████████████████████████████████████▊                                | 13/23 [08:17<06:14, 37.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  61%|█████████████████████████████████████████████                             | 14/23 [08:56<05:41, 37.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  65%|████████████████████████████████████████████████▎                         | 15/23 [09:36<05:07, 38.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  70%|███████████████████████████████████████████████████▍                      | 16/23 [09:59<03:57, 33.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  74%|██████████████████████████████████████████████████████▋                   | 17/23 [10:38<03:32, 35.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  78%|█████████████████████████████████████████████████████████▉                | 18/23 [11:03<02:41, 32.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  83%|█████████████████████████████████████████████████████████████▏            | 19/23 [11:34<02:06, 31.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  87%|████████████████████████████████████████████████████████████████▎         | 20/23 [12:13<01:41, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  91%|███████████████████████████████████████████████████████████████████▌      | 21/23 [12:51<01:10, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral):  96%|██████████████████████████████████████████████████████████████████████▊   | 22/23 [13:30<00:36, 36.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (mistral): 100%|██████████████████████████████████████████████████████████████████████████| 23/23 [14:11<00:00, 37.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_medium_24m15s.txt (mistral) done in 851.3 sec (23 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):   0%|                                                                                     | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):   4%|███▎                                                                         | 1/23 [00:38<14:12, 38.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):   9%|██████▋                                                                      | 2/23 [01:17<13:33, 38.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  13%|██████████                                                                   | 3/23 [01:55<12:50, 38.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  17%|█████████████▍                                                               | 4/23 [02:29<11:39, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  22%|████████████████▋                                                            | 5/23 [03:08<11:16, 37.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  26%|████████████████████                                                         | 6/23 [03:48<10:48, 38.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  30%|███████████████████████▍                                                     | 7/23 [04:27<10:15, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  35%|██████████████████████████▊                                                  | 8/23 [05:06<09:41, 38.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  39%|██████████████████████████████▏                                              | 9/23 [05:45<09:04, 38.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  43%|█████████████████████████████████                                           | 10/23 [06:25<08:27, 39.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  48%|████████████████████████████████████▎                                       | 11/23 [06:45<06:39, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  52%|███████████████████████████████████████▋                                    | 12/23 [07:28<06:39, 36.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  57%|██████████████████████████████████████████▉                                 | 13/23 [08:09<06:15, 37.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  61%|██████████████████████████████████████████████▎                             | 14/23 [08:49<05:44, 38.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  65%|█████████████████████████████████████████████████▌                          | 15/23 [09:29<05:10, 38.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  70%|████████████████████████████████████████████████████▊                       | 16/23 [10:00<04:15, 36.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  74%|████████████████████████████████████████████████████████▏                   | 17/23 [10:39<03:44, 37.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  78%|███████████████████████████████████████████████████████████▍                | 18/23 [11:01<02:43, 32.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  83%|██████████████████████████████████████████████████████████████▊             | 19/23 [11:41<02:19, 34.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  87%|██████████████████████████████████████████████████████████████████          | 20/23 [12:20<01:48, 36.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  91%|█████████████████████████████████████████████████████████████████████▍      | 21/23 [12:59<01:13, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral):  96%|████████████████████████████████████████████████████████████████████████▋   | 22/23 [13:38<00:37, 37.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (mistral): 100%|████████████████████████████████████████████████████████████████████████████| 23/23 [13:48<00:00, 36.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_large_33m5s.txt (mistral) done in 828.8 sec (23 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):   0%|                                                                                     | 0/42 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):   2%|█▊                                                                           | 1/42 [00:16<11:32, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):   5%|███▋                                                                         | 2/42 [00:58<20:53, 31.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):   7%|█████▌                                                                       | 3/42 [01:39<23:21, 35.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  10%|███████▎                                                                     | 4/42 [02:19<23:48, 37.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  12%|█████████▏                                                                   | 5/42 [03:01<24:01, 38.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  14%|███████████                                                                  | 6/42 [03:42<23:44, 39.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  17%|████████████▊                                                                | 7/42 [04:20<22:58, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  19%|██████████████▋                                                              | 8/42 [05:00<22:15, 39.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  21%|████████████████▌                                                            | 9/42 [05:39<21:32, 39.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  24%|██████████████████                                                          | 10/42 [06:18<20:56, 39.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  26%|███████████████████▉                                                        | 11/42 [06:57<20:17, 39.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  29%|█████████████████████▋                                                      | 12/42 [07:19<16:57, 33.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  31%|███████████████████████▌                                                    | 13/42 [07:58<17:09, 35.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  33%|█████████████████████████▎                                                  | 14/42 [08:38<17:15, 36.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  36%|███████████████████████████▏                                                | 15/42 [08:59<14:24, 32.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  38%|████████████████████████████▉                                               | 16/42 [09:43<15:24, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  40%|██████████████████████████████▊                                             | 17/42 [10:24<15:29, 37.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  43%|████████████████████████████████▌                                           | 18/42 [10:52<13:45, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  45%|██████████████████████████████████▍                                         | 19/42 [11:33<13:58, 36.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  48%|████████████████████████████████████▏                                       | 20/42 [12:18<14:18, 39.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  50%|██████████████████████████████████████                                      | 21/42 [13:09<14:54, 42.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  52%|███████████████████████████████████████▊                                    | 22/42 [13:27<11:43, 35.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  55%|█████████████████████████████████████████▌                                  | 23/42 [14:05<11:24, 36.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  57%|███████████████████████████████████████████▍                                | 24/42 [14:30<09:47, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  60%|█████████████████████████████████████████████▏                              | 25/42 [14:55<08:40, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  62%|███████████████████████████████████████████████                             | 26/42 [15:35<08:50, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  64%|████████████████████████████████████████████████▊                           | 27/42 [16:11<08:32, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  67%|██████████████████████████████████████████████████▋                         | 28/42 [16:51<08:22, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  69%|████████████████████████████████████████████████████▍                       | 29/42 [17:28<07:51, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  71%|██████████████████████████████████████████████████████▎                     | 30/42 [18:06<07:19, 36.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  74%|████████████████████████████████████████████████████████                    | 31/42 [18:47<06:59, 38.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  76%|█████████████████████████████████████████████████████████▉                  | 32/42 [19:27<06:26, 38.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  79%|███████████████████████████████████████████████████████████▋                | 33/42 [20:06<05:47, 38.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  81%|█████████████████████████████████████████████████████████████▌              | 34/42 [20:45<05:11, 38.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  83%|███████████████████████████████████████████████████████████████▎            | 35/42 [21:26<04:35, 39.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  86%|█████████████████████████████████████████████████████████████████▏          | 36/42 [22:05<03:56, 39.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  88%|██████████████████████████████████████████████████████████████████▉         | 37/42 [22:44<03:15, 39.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  90%|████████████████████████████████████████████████████████████████████▊       | 38/42 [23:23<02:36, 39.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  93%|██████████████████████████████████████████████████████████████████████▌     | 39/42 [23:51<01:47, 35.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  95%|████████████████████████████████████████████████████████████████████████▍   | 40/42 [24:29<01:12, 36.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral):  98%|██████████████████████████████████████████████████████████████████████████▏ | 41/42 [24:45<00:30, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (mistral): 100%|████████████████████████████████████████████████████████████████████████████| 42/42 [24:52<00:00, 35.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_tiny_14m19s.txt (mistral) done in 1493.0 sec (42 chunks)\n",
      "> Loading qwen (Qwen/Qwen1.5-7B-Chat) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960271ed9910453586df43b4eb5c9f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3cf09308c9436db10b5bd4bbddce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ac926f139d4c18a9988ea14ae5a824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04e9318ab1b406186248f1921f57a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44934eb1bddb4e2cad2099268cb30bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5652adcd9a45568f71aca06aaa9cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/31.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cda28ffe7df4f0e86dcc47677fe76c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11954a982d7b4f0594fd53deed08bf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044aa79941bd452f83a1898c8d57d8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdb92162182496f9b7136ba70a48c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62cb047613664af3aabba189a7e8f935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef446bfc2834ed0a83500d0f8de0228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5562512b44554ec0a718c1ae5143d172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):   0%|                                                                                       | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):   5%|███▊                                                                           | 1/21 [00:00<00:17,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  10%|███████▌                                                                       | 2/21 [00:01<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  14%|███████████▎                                                                   | 3/21 [00:02<00:14,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  19%|███████████████                                                                | 4/21 [00:03<00:16,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  24%|██████████████████▊                                                            | 5/21 [00:32<02:57, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  29%|██████████████████████▌                                                        | 6/21 [00:50<03:18, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  33%|██████████████████████████▎                                                    | 7/21 [00:50<02:08,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  38%|██████████████████████████████                                                 | 8/21 [00:51<01:24,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  43%|█████████████████████████████████▊                                             | 9/21 [01:02<01:35,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  48%|█████████████████████████████████████▏                                        | 10/21 [01:03<01:03,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  52%|████████████████████████████████████████▊                                     | 11/21 [01:04<00:42,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  57%|████████████████████████████████████████████▌                                 | 12/21 [01:05<00:29,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  62%|████████████████████████████████████████████████▎                             | 13/21 [01:29<01:15,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  67%|████████████████████████████████████████████████████                          | 14/21 [01:32<00:52,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  71%|███████████████████████████████████████████████████████▋                      | 15/21 [01:43<00:52,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  76%|███████████████████████████████████████████████████████████▍                  | 16/21 [01:45<00:32,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  81%|███████████████████████████████████████████████████████████████▏              | 17/21 [02:00<00:36,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  86%|██████████████████████████████████████████████████████████████████▊           | 18/21 [02:02<00:20,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  90%|██████████████████████████████████████████████████████████████████████▌       | 19/21 [02:02<00:09,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen):  95%|██████████████████████████████████████████████████████████████████████████▎   | 20/21 [02:03<00:03,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_small_20m20s.txt (qwen): 100%|██████████████████████████████████████████████████████████████████████████████| 21/21 [02:04<00:00,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_small_20m20s.txt (qwen) done in 124.0 sec (21 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):   0%|                                                                                      | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):   6%|████▎                                                                         | 1/18 [00:16<04:43, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  11%|████████▋                                                                     | 2/18 [00:42<05:57, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  17%|█████████████                                                                 | 3/18 [00:43<03:05, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  22%|█████████████████▎                                                            | 4/18 [00:45<01:54,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  28%|█████████████████████▋                                                        | 5/18 [00:58<02:11, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  33%|██████████████████████████                                                    | 6/18 [01:17<02:36, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  39%|██████████████████████████████▎                                               | 7/18 [01:34<02:37, 14.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  44%|██████████████████████████████████▋                                           | 8/18 [01:35<01:39,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  50%|███████████████████████████████████████                                       | 9/18 [01:54<01:54, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  56%|██████████████████████████████████████████▊                                  | 10/18 [01:56<01:16,  9.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  61%|███████████████████████████████████████████████                              | 11/18 [01:58<00:51,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  67%|███████████████████████████████████████████████████▎                         | 12/18 [02:00<00:33,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  72%|███████████████████████████████████████████████████████▌                     | 13/18 [02:29<01:03, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  78%|███████████████████████████████████████████████████████████▉                 | 14/18 [02:29<00:36,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  83%|████████████████████████████████████████████████████████████████▏            | 15/18 [02:33<00:22,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  89%|████████████████████████████████████████████████████████████████████▍        | 16/18 [02:35<00:11,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen):  94%|████████████████████████████████████████████████████████████████████████▋    | 17/18 [02:35<00:04,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_medium_24m15s.txt (qwen): 100%|█████████████████████████████████████████████████████████████████████████████| 18/18 [02:37<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_medium_24m15s.txt (qwen) done in 157.6 sec (18 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):   0%|                                                                                        | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):   6%|████▍                                                                           | 1/18 [00:01<00:31,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  11%|████████▉                                                                       | 2/18 [00:03<00:29,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  17%|█████████████▎                                                                  | 3/18 [00:05<00:24,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  22%|█████████████████▊                                                              | 4/18 [00:05<00:16,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  28%|██████████████████████▏                                                         | 5/18 [00:08<00:24,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  33%|██████████████████████████▋                                                     | 6/18 [00:09<00:18,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  39%|███████████████████████████████                                                 | 7/18 [00:12<00:20,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  44%|███████████████████████████████████▌                                            | 8/18 [00:13<00:17,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  50%|████████████████████████████████████████                                        | 9/18 [00:14<00:13,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  56%|███████████████████████████████████████████▉                                   | 10/18 [00:15<00:10,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  61%|████████████████████████████████████████████████▎                              | 11/18 [00:16<00:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  67%|████████████████████████████████████████████████████▋                          | 12/18 [00:18<00:09,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  72%|█████████████████████████████████████████████████████████                      | 13/18 [00:19<00:07,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  78%|█████████████████████████████████████████████████████████████▍                 | 14/18 [00:20<00:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  83%|█████████████████████████████████████████████████████████████████▊             | 15/18 [00:23<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  89%|██████████████████████████████████████████████████████████████████████▏        | 16/18 [00:23<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen):  94%|██████████████████████████████████████████████████████████████████████████▌    | 17/18 [00:24<00:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_large_33m5s.txt (qwen): 100%|███████████████████████████████████████████████████████████████████████████████| 18/18 [00:37<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_large_33m5s.txt (qwen) done in 37.1 sec (18 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):   0%|                                                                                        | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):   3%|██▎                                                                             | 1/35 [00:00<00:24,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):   6%|████▌                                                                           | 2/35 [00:03<00:58,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):   9%|██████▊                                                                         | 3/35 [00:22<05:06,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  11%|█████████▏                                                                      | 4/35 [00:23<03:18,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  14%|███████████▍                                                                    | 5/35 [00:24<02:09,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  17%|█████████████▋                                                                  | 6/35 [00:54<06:25, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  20%|████████████████                                                                | 7/35 [00:55<04:16,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  23%|██████████████████▎                                                             | 8/35 [00:56<02:56,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  26%|████████████████████▌                                                           | 9/35 [01:25<05:49, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  29%|██████████████████████▌                                                        | 10/35 [01:39<05:41, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  31%|████████████████████████▊                                                      | 11/35 [01:49<04:59, 12.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  34%|███████████████████████████                                                    | 12/35 [01:49<03:23,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  37%|█████████████████████████████▎                                                 | 13/35 [02:21<05:46, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  40%|███████████████████████████████▌                                               | 14/35 [02:21<03:54, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  43%|█████████████████████████████████▊                                             | 15/35 [02:52<05:43, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  46%|████████████████████████████████████                                           | 16/35 [02:53<03:52, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  49%|██████████████████████████████████████▎                                        | 17/35 [03:22<05:10, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  51%|████████████████████████████████████████▋                                      | 18/35 [03:23<03:28, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  54%|██████████████████████████████████████████▉                                    | 19/35 [03:29<02:48, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  57%|█████████████████████████████████████████████▏                                 | 20/35 [03:30<01:54,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  60%|███████████████████████████████████████████████▍                               | 21/35 [03:47<02:24, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  63%|█████████████████████████████████████████████████▋                             | 22/35 [04:16<03:28, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  66%|███████████████████████████████████████████████████▉                           | 23/35 [04:32<03:10, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  69%|██████████████████████████████████████████████████████▏                        | 24/35 [04:32<02:04, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  71%|████████████████████████████████████████████████████████▍                      | 25/35 [05:02<02:49, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  74%|██████████████████████████████████████████████████████████▋                    | 26/35 [05:03<01:48, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  77%|████████████████████████████████████████████████████████████▉                  | 27/35 [05:21<01:51, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  80%|███████████████████████████████████████████████████████████████▏               | 28/35 [05:51<02:11, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  83%|█████████████████████████████████████████████████████████████████▍             | 29/35 [05:52<01:19, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  86%|███████████████████████████████████████████████████████████████████▋           | 30/35 [05:52<00:47,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  89%|█████████████████████████████████████████████████████████████████████▉         | 31/35 [06:23<01:03, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  91%|████████████████████████████████████████████████████████████████████████▏      | 32/35 [06:25<00:34, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  94%|██████████████████████████████████████████████████████████████████████████▍    | 33/35 [06:25<00:16,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen):  97%|████████████████████████████████████████████████████████████████████████████▋  | 34/35 [06:26<00:05,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "input_ids: cuda:0\n",
      "attention_mask: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "recognized_tiny_14m19s.txt (qwen): 100%|███████████████████████████████████████████████████████████████████████████████| 35/35 [06:26<00:00, 11.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> recognized_tiny_14m19s.txt (qwen) done in 386.9 sec (35 chunks)\n",
      "> Loading llama (meta-llama/Llama-2-7b-chat-hf) ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-6816bc29-54285b27730255e442d2a8b7;61ed4734-8984-403f-b44c-5ef614a86b2b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-6816bc29-54285b27730255e442d2a8b7;61ed4734-8984-403f-b44c-5ef614a86b2b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_key, model_name \u001b[38;5;129;01min\u001b[39;00m MODELS\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     93\u001b[0m         model_name,\n\u001b[1;32m     94\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     95\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,        \u001b[38;5;66;03m# for Qwen especially\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:877\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    875\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1017\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1015\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1017\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1019\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m~/tts/venv/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-6816bc29-54285b27730255e442d2a8b7;61ed4734-8984-403f-b44c-5ef614a86b2b)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"removed\"\n",
    "\n",
    "TRANSCRIPTS_PATH = \"transcripts\"\n",
    "OUTPUT_PATH = \"summaries_russian_llm\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "MODELS = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"qwen\": \"Qwen/Qwen1.5-7B-Chat\",\n",
    "    \"llama\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"mistral\": \"Сделай краткое и информативное резюме следующего текста собрания на русском языке, выдели принятые решения и action-items.:\\n\\n{}\",\n",
    "    \"llama\": \"Сделай краткое и информативное резюме следующего текста собрания на русском языке, выдели принятые решения и action-items.:\\n\\n{}\",\n",
    "    \"qwen\": \"Сделай краткое и информативное резюме следующего текста собрания на русском языке, выдели принятые решения и action-items.:\\n\\n{}\",\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CHUNK_TOKENS = 2048\n",
    "GENERATE_TOKENS = 512\n",
    "\n",
    "def chunk_text(text, tokenizer, max_tokens):\n",
    "    \"\"\"Splits Russian text into token-length chunks at sentence boundaries.\"\"\"\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    sentences = sent_tokenize(text, language='russian')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in sentences:\n",
    "        if len(tokenizer.encode(current_chunk + \" \" + sent)) < max_tokens:\n",
    "            current_chunk += \" \" + sent\n",
    "        else:\n",
    "            if current_chunk: chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def generate_summary(\n",
    "    model, tokenizer, prompt, device, chunk_tokens=2048, gen_tokens=512\n",
    "):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=chunk_tokens,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # print(\"Model device:\", next(model.parameters()).device)\n",
    "    # print(\"input_ids:\", input_ids.device)\n",
    "    # print(\"attention_mask:\", attention_mask.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=chunk_tokens+gen_tokens,\n",
    "            temperature=0.3,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    result = tokenizer.decode(\n",
    "        output_ids[0][input_ids.shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "timing_report = []\n",
    "for model_key, model_name in MODELS.items():\n",
    "    print(f\"> Loading {model_key} ({model_name}) ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=None,\n",
    "        trust_remote_code=True,        # for Qwen especially\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    prompt_template = PROMPT_TEMPLATES[model_key]\n",
    "    for fname in os.listdir(TRANSCRIPTS_PATH):\n",
    "        if not fname.endswith('.txt'):\n",
    "            continue\n",
    "        transcript_path = os.path.join(TRANSCRIPTS_PATH, fname)\n",
    "        outname = os.path.join(OUTPUT_PATH, f\"{fname}.{model_key}.summary.txt\")\n",
    "        if os.path.exists(outname):\n",
    "            print(f\"Skip {outname}, exists.\")\n",
    "            continue\n",
    "        with open(transcript_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            text = fin.read()\n",
    "        chunks = chunk_text(text, tokenizer, CHUNK_TOKENS)\n",
    "        summaries = []\n",
    "        t0 = time.time()\n",
    "        for chunk in tqdm(chunks, desc=f\"{fname} ({model_key})\"):\n",
    "            prompt = prompt_template.format(chunk)\n",
    "            summary = generate_summary(model, tokenizer, prompt, DEVICE)\n",
    "            summaries.append(summary)\n",
    "        final_summary = \"\\n\".join(summaries)\n",
    "        elapsed = time.time() - t0\n",
    "        with open(outname, \"w\", encoding=\"utf-8\") as fout:\n",
    "            fout.write(final_summary)\n",
    "        print(f\">>> {fname} ({model_key}) done in {elapsed:.1f} sec ({len(chunks)} chunks)\")\n",
    "        timing_report.append({'filename': fname, 'model': model_key, 'seconds': elapsed})\n",
    "\n",
    "import csv\n",
    "with open(os.path.join(OUTPUT_PATH, 'timing_report.csv'), \"w\", encoding=\"utf-8\", newline='') as f_csv:\n",
    "    writer = csv.DictWriter(f_csv, fieldnames=['filename', 'model', 'seconds'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(timing_report)\n",
    "print(\"Timing report written:\", os.path.join(OUTPUT_PATH, 'timing_report.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b8cc92c-9499-439f-9d5e-40416763f3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f80cb47db24cde8823e9df6aba5bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Хорошо, мне нужно сделать краткое и информативное резюме собрания на русском языке, выделив принятые решения и action-items. Начну с того, что внимательно прочитаю текст, чтобы понять основные моменты.\n",
      "\n",
      "Сначала текст говорит о важности искусственного интеллекта (ИИ) как ключевого направления для будущего продуктов. Важно отметить, что некоторые коллеги поднимают вопросы, связанные с внедрением ИИ, и нужно совместно их решать. Также упоминается возможное создание нового управления для ускорения внедрения ИИ. \n",
      "\n",
      "Далее, говорится о том, что ИИ может ускорить развитие, как это происходило в истории человечества через появление письменности. Примеры: чтение большого количества текстов и обучение на них. ИИ способен обрабатывать огромные объемы данных быстрее человека, что может привести к трансформации подходов к жизни и бизнесу. \n",
      "\n",
      "Важно подчеркнуть, что компания должна активно внедрять ИИ, чтобы не отставать от конкурентов. Также упоминается, что нужно сформировать план реализации запросов и выделить направления, которые мешают внедрению ИИ. \n",
      "\n",
      "Есть упоминание о том, что в компании есть люди, которые сомневаются в эффективности ИИ из-за недостатка готовых кейсов. Нужно учитывать это и возможно, разработать примеры применения ИИ в продуктовой линейке, например, в QueryTouchCity. \n",
      "\n",
      "Также есть упоминание о том, что продукт развивается эволюционно, а не революционно, и нужно повышать технологическую зрелость через усложнение алгоритмов и методик. \n",
      "\n",
      "Теперь нужно выделить принятые решения и action-items. Принятые решения: необходимость внедрения ИИ, создание нового управления, формирование плана реализации, анализ направлений, мешающих внедрению. Action-items: сформировать план реализации, выделить ключевые направления, разработать примеры применения ИИ в продуктах, повысить технологическую зрелость, учесть сомнения коллег. \n",
      "\n",
      "Нужно убедиться, что резюме краткое, структурированное, и выделены основные моменты. Проверю, нет ли пропущенных важных деталей, и оформлю это в виде четкого резюме с разделами \"Принятые решения\" и \"Action-items\".\n",
      "</think>\n",
      "content: **Краткое резюме собрания:**  \n",
      "На собрании обсуждались ключевые направления внедрения искусственного интеллекта (ИИ) в продукты компании, а также вызовы, сопутствующие его внедрению. Участники отметили, что ИИ может стать фундаментом для трансформации подходов к бизнесу и жизни, аналогично тому, как письменность ускорила развитие человечества. Однако в компании существуют сомнения из-за отсутствия готовых прикладных кейсов и недостаточного понимания потенциала ИИ.  \n",
      "\n",
      "**Принятые решения:**  \n",
      "1. **Внедрение ИИ как приоритет**: Необходимость активного внедрения ИИ в продукты, включая QueryTouchCity, для ускорения развития и конкурентоспособности.  \n",
      "2. **Создание нового управления**: Возможно, создание специального отдела или управления для координации и ускорения внедрения ИИ.  \n",
      "3. **Формирование плана реализации**: Сформировать план действий для реализации запросов, выделить ключевые направления, которые мешают внедрению ИИ.  \n",
      "4. **Повышение технологической зрелости**: Усложнение алгоритмов и методик ИИ без массового разработки, с акцентом на эволюционное развитие продукта.  \n",
      "\n",
      "**Action-items:**  \n",
      "- Сформировать детальный план внедрения ИИ в продукты, включая QueryTouchCity.  \n",
      "- Выделить и проанализировать направления, которые являются барьерами для внедрения ИИ.  \n",
      "- Разработать примеры применения ИИ в продуктовой линейке, чтобы убедить сомневающихся в его эффективности.  \n",
      "- Повысить технологическую зрелость ИИ через улучшение алгоритмов и методик.  \n",
      "- Провести дополнительные обсуждения с коллегами, чтобы учесть их вопросы и сомнения.  \n",
      "\n",
      "**Итог:** Компания должна активно развивать ИИ, чтобы не отставать от конкурентов, а также проработать стратегию внедрения и устранить возможные препятствия.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "with open(\"transcripts/recognized_large_33m5s.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "text = text[:5000]\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Сделай краткое и информативное резюме следующего текста собрания на русском языке, выдели принятые решения и action-items.:\\n\\n\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt + text}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048 #32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dece79-a0d6-4d74-b4ba-4aaa3b4f9613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36be213e-3107-46dc-bffe-2cf438c7673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52a488c8ec4a8f8782ef900c7c95df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked transcript into 3 segments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkudriashov/tts/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/mkudriashov/tts/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/mkudriashov/tts/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Summarizing chunks: 100%|█████████████████████████████████████████████████████████████████████| 3/3 [08:33<00:00, 171.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final summary written to summaries_russian_llm/qwen3_summary_summary_global.txt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "model_name = \"Qwen/Qwen3-8B\"       # << DO NOT CHANGE THIS\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# INPUT_PATH = \"transcripts/recognized_large_33m5s.txt\"\n",
    "\n",
    "INPUT_PATH = \"summaries_russian_llm/qwen3_summary_summary.txt\"\n",
    "OUTPUT_PATH = \"summaries_russian_llm/qwen3_summary_summary_global.txt\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "PROMPT_CHUNK = (\n",
    "    \"Сделай краткое и информативное резюме следующего текста собрания на русском языке, \"\n",
    "    \"выдели принятые решения и action-items:\\n\\n\"\n",
    ")\n",
    "\n",
    "PROMPT_GLOBAL = (\n",
    "    \"Вот несколько кратких резюме разных частей собрания. На их основе создай обобщённое, \"\n",
    "    \"сжатое итоговое резюме всего собрания, выдели только основные решения и важнейшие action items:\\n\\n\"\n",
    ")\n",
    "\n",
    "def chunk_text(text, tokenizer, max_tokens=2000):\n",
    "    sentences = sent_tokenize(text, language='russian')\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for sent in sentences:\n",
    "        test = (current + \" \" + sent).strip()\n",
    "        token_count = len(tokenizer.encode(test))\n",
    "        if token_count < max_tokens:\n",
    "            current = test\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            current = sent\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    return chunks\n",
    "\n",
    "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunks = chunk_text(text, tokenizer, max_tokens=2000)\n",
    "print(f\"Chunked transcript into {len(chunks)} segments.\")\n",
    "\n",
    "summaries = []\n",
    "for idx, chunk in enumerate(tqdm(chunks, desc=\"Summarizing chunks\")):\n",
    "    messages = [{\"role\": \"user\", \"content\": PROMPT_GLOBAL + chunk}]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([chat_text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    summary_chunk = tokenizer.decode(\n",
    "        out_ids[0][model_inputs.input_ids.shape[-1]:], skip_special_tokens=True\n",
    "    ).strip()\n",
    "    summaries.append(summary_chunk)\n",
    "\n",
    "# ---- Second pass: summarize the summaries ----\n",
    "global_summary_prompt = PROMPT_GLOBAL + \"\\n\\n\".join(summaries)\n",
    "# messages = [{\"role\": \"user\", \"content\": global_summary_prompt}]\n",
    "# chat_text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([chat_text], return_tensors=\"pt\").to(model.device)\n",
    "# with torch.no_grad():\n",
    "#     out_ids = model.generate(\n",
    "#         **model_inputs,\n",
    "#         max_new_tokens=700,\n",
    "#         temperature=0.1,\n",
    "#         do_sample=False,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "#         eos_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "# final_summary = tokenizer.decode(\n",
    "#     out_ids[0][model_inputs.input_ids.shape[-1]:], skip_special_tokens=True\n",
    "# ).strip()\n",
    "\n",
    "final_summary = global_summary_prompt\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as fout:\n",
    "    fout.write(final_summary)\n",
    "\n",
    "print(f\"\\nFinal summary written to {OUTPUT_PATH}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d4227f-5112-4d85-a701-160cce4324c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
